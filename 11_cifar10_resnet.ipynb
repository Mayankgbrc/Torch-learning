{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ecd086-b2dc-45a3-9c83-da9cebc25f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "from timm.data.mixup import Mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ce0b039-685a-490a-adfc-eb6f7fce9232",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70211a5f-d234-42fa-8bbf-0175ae900aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentation and normalization (using ImageNet statistics)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(224),                     # Resize CIFAR-10 images to 224x224\n",
    "    transforms.RandomCrop(224, padding=4),      # Random crop with padding\n",
    "    transforms.RandomHorizontalFlip(),          # Random horizontal flip\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
    "                         std=[0.229, 0.224, 0.225])   # ImageNet std\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(224),                     # Resize for consistency\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a21bd13e-f130-4eb6-8ee1-947fccc1f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.CIFAR10(root = './data', download = True, train = True, transform = transform_train)\n",
    "test_data = torchvision.datasets.CIFAR10(root = './data', download = True, train = False, transform = transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e7d2282-eeb8-47d8-abd1-48a672f613b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_batches = DataLoader(train_data, shuffle = True, batch_size = 8, pin_memory=True, num_workers = 2)\n",
    "test_data_batches = DataLoader(test_data, shuffle = True, batch_size = 8, pin_memory=True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5d546bb-ce32-4731-95f7-7b60730f2806",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ef537f1-8b2d-4efd-9894-d81656868f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = model.fc.in_features\n",
    "model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "589f376b-b09b-42f8-96a7-642d97b19aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(num_features, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be3bd20e-378f-40b8-a8a3-e2a0b636ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, freeze all layers except the final fully connected layer:\n",
    "for name, param in model.named_parameters():\n",
    "    # Freeze everything except layers in the last block (or the final two layers)\n",
    "    if \"layer4\" not in name and \"fc\" not in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6cf19f8c-c1e6-45c9-87a7-3118be408d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 0.7214\n",
      "Epoch [2/30], Loss: 0.4353\n",
      "Epoch [3/30], Loss: 0.3639\n",
      "Epoch [4/30], Loss: 0.3168\n",
      "Epoch [5/30], Loss: 0.2773\n",
      "Epoch [6/30], Loss: 0.2448\n",
      "Epoch [7/30], Loss: 0.2220\n",
      "Epoch [8/30], Loss: 0.1991\n",
      "Epoch [9/30], Loss: 0.1816\n",
      "Epoch [10/30], Loss: 0.1626\n",
      "Epoch [11/30], Loss: 0.1519\n",
      "Epoch [12/30], Loss: 0.1357\n",
      "Epoch [13/30], Loss: 0.1192\n",
      "Epoch [14/30], Loss: 0.1146\n",
      "Epoch [15/30], Loss: 0.1035\n",
      "Epoch [16/30], Loss: 0.0941\n",
      "Epoch [17/30], Loss: 0.0854\n",
      "Epoch [18/30], Loss: 0.0790\n",
      "Epoch [19/30], Loss: 0.0745\n",
      "Epoch [20/30], Loss: 0.0700\n",
      "Epoch [21/30], Loss: 0.0630\n",
      "Epoch [22/30], Loss: 0.0612\n",
      "Epoch [23/30], Loss: 0.0562\n",
      "Epoch [24/30], Loss: 0.0545\n",
      "Epoch [25/30], Loss: 0.0505\n",
      "Epoch [26/30], Loss: 0.0479\n",
      "Epoch [27/30], Loss: 0.0442\n",
      "Epoch [28/30], Loss: 0.0425\n",
      "Epoch [29/30], Loss: 0.0416\n",
      "Epoch [30/30], Loss: 0.0361\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': model.fc.parameters(), 'lr': 3e-4},\n",
    "    {'params': [param for name, param in model.named_parameters() if \"layer4\" in name], 'lr': 1e-4}\n",
    "], weight_decay=1e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=3e-4, steps_per_epoch=len(train_data_batches), epochs=epochs\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_data_batches):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(train_data_batches)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7dfc0752-9cfd-4c77-8b85-d651161d0096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 93.81%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "93.81"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "def check_accuracy(model, batch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in batch:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    test_acc = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "    return test_acc\n",
    "\n",
    "check_accuracy(model, test_data_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4188571f-f6a5-4f1d-a583-f8b819237435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-3.8974e-02,  2.9298e-02, -1.3958e-02,  ..., -4.6387e-02,\n",
       "          -1.2887e-02,  1.2016e-02],\n",
       "         [-1.0419e-01,  3.4756e-02,  6.4834e-02,  ...,  3.3782e-02,\n",
       "          -6.7517e-02,  2.9924e-02],\n",
       "         [ 3.4154e-02,  5.3276e-03,  5.3004e-02,  ..., -7.9814e-02,\n",
       "           3.5800e-02,  5.7835e-02],\n",
       "         ...,\n",
       "         [-9.1951e-03, -8.7283e-02,  2.6646e-02,  ...,  2.1281e-02,\n",
       "           4.0279e-03, -6.6018e-02],\n",
       "         [-1.2005e-01, -5.0391e-02,  5.4703e-02,  ...,  6.6629e-02,\n",
       "           6.4773e-02,  3.9974e-02],\n",
       "         [-8.2902e-02,  6.2400e-03, -9.1404e-02,  ..., -3.7850e-03,\n",
       "          -3.6153e-02, -1.1456e-04]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0363,  0.0100,  0.0260,  0.0085,  0.0251,  0.0221, -0.0104,  0.0187,\n",
       "         -0.0086, -0.0113], device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.fc.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5daf000-a6eb-4268-b3d4-ef4c2ed45edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "bn1.weight\n",
      "bn1.bias\n",
      "layer1.0.conv1.weight\n",
      "layer1.0.bn1.weight\n",
      "layer1.0.bn1.bias\n",
      "layer1.0.conv2.weight\n",
      "layer1.0.bn2.weight\n",
      "layer1.0.bn2.bias\n",
      "layer1.1.conv1.weight\n",
      "layer1.1.bn1.weight\n",
      "layer1.1.bn1.bias\n",
      "layer1.1.conv2.weight\n",
      "layer1.1.bn2.weight\n",
      "layer1.1.bn2.bias\n",
      "layer2.0.conv1.weight\n",
      "layer2.0.bn1.weight\n",
      "layer2.0.bn1.bias\n",
      "layer2.0.conv2.weight\n",
      "layer2.0.bn2.weight\n",
      "layer2.0.bn2.bias\n",
      "layer2.0.downsample.0.weight\n",
      "layer2.0.downsample.1.weight\n",
      "layer2.0.downsample.1.bias\n",
      "layer2.1.conv1.weight\n",
      "layer2.1.bn1.weight\n",
      "layer2.1.bn1.bias\n",
      "layer2.1.conv2.weight\n",
      "layer2.1.bn2.weight\n",
      "layer2.1.bn2.bias\n",
      "layer3.0.conv1.weight\n",
      "layer3.0.bn1.weight\n",
      "layer3.0.bn1.bias\n",
      "layer3.0.conv2.weight\n",
      "layer3.0.bn2.weight\n",
      "layer3.0.bn2.bias\n",
      "layer3.0.downsample.0.weight\n",
      "layer3.0.downsample.1.weight\n",
      "layer3.0.downsample.1.bias\n",
      "layer3.1.conv1.weight\n",
      "layer3.1.bn1.weight\n",
      "layer3.1.bn1.bias\n",
      "layer3.1.conv2.weight\n",
      "layer3.1.bn2.weight\n",
      "layer3.1.bn2.bias\n",
      "layer4.0.conv1.weight\n",
      "layer4.0.bn1.weight\n",
      "layer4.0.bn1.bias\n",
      "layer4.0.conv2.weight\n",
      "layer4.0.bn2.weight\n",
      "layer4.0.bn2.bias\n",
      "layer4.0.downsample.0.weight\n",
      "layer4.0.downsample.1.weight\n",
      "layer4.0.downsample.1.bias\n",
      "layer4.1.conv1.weight\n",
      "layer4.1.bn1.weight\n",
      "layer4.1.bn1.bias\n",
      "layer4.1.conv2.weight\n",
      "layer4.1.bn2.weight\n",
      "layer4.1.bn2.bias\n",
      "fc.weight\n",
      "fc.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5087b9-7f82-49d4-8d54-1c84de4e66d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
